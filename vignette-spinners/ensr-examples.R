#'---
#'title: "Elastic Net SearcheR Examples"
#'subtitle: "ensr package version `r packageVersion('ensr')`"
#'author: "Peter E. DeWitt"
#'date: "`r Sys.Date()`"
#'output:
#'  rmarkdown::html_vignette
#'bibliography: references.bib
#'vignette: >
#'  %\VignetteEngine{knitr::rmarkdown}
#'  %\VignetteIndexEntry{ensr-examples}
#'  %\VignetteEncoding{UTF-8}
#'---

#+ label=setup, include = FALSE
library(knitr)
knitr::opts_chunk$set(collapse = TRUE, eval = F)
options(qwraps2_markup = "markdown")

#'
# /*
# =============================================================================
# */
#'
#' The primary purpose of the ensr package is to provide methods for searching
#' for preferable values of $\lambda$ and $\alpha$ in elastice net regression.
#' This This vignette starts with a
#' summary of elastic net regression and the use/limitations of the
#' [glmnet](https://cran.r-studio.com/package=glmnet) pacakge.  Examples of data
#' set preperation follow and the vignette concludes with elastic net regression
#' results.
#'
#+label="load_and_attach_ensr"
library(ensr)
library(data.table)
library(microbenchmark)
options(datatable.print.topn  = 3L,
        datatable.print.nrows = 3L)

#'
#' # Elastic Net Regression
#'
#' Elastic Net Regression [@friedman2010regularization] is a penalized linear
#' modeling approach that is a mixture of ridge regression [@hoerl1970ridge],
#' and lasso regression [@tibshirani1996regression].  Ridge regression
#' reducing the impact of co-linearity, and lasso
#' reduces the dimensionality of the support by shrinking some of the regression
#' coefficients to zero.  Elastic net does both of these by solving the
#' following equation for gaussian responses:
#' $$\min_{\beta_0, \beta \in \mathbb{R}^{p+1}} \frac{1}{2N} \sum_{i = 1}^{N}
#' \left( y_i - \beta_0 - x_i^T \beta \right)^2 + \lambda \left[ \left(1 -
#' \alpha \right) \frac{\left \lVert \beta \right \rVert_{2}^{2}}{2} + \alpha
#' \left \lVert \beta \right \rVert_{1} \right],$$
#' where $\lambda \geq 0$ is the complexity parameter and $0 \leq \alpha \leq 1$
#' is the compromise between ridge $\left(\alpha = 0\right)$ and lasso $\left(
#' \alpha = 1 \right).$
#'
#' One advantage for $0 < \alpha < 1,$ compared to lasso is that elastic net
#' will consistently return the same set of non-zero coefficients when some of
#' the predictors are highly correlated.  Lasso, $\alpha = 1,$ has the draw back
#' of returning differing sets of non-zero coefficients when highly correlated
#' predictors are in the model.
#'
#' A benefit of elastic net regression is that the $\beta$ vector can easily be
#' programmed into any other software package capable of matrix, or even simple,
#' arithmetic.  So, while a GBM model might be a better fit of the data than the
#' linear model found via elastic net, GBM models are extremely difficult, or
#' impossible to export to other tools.
#'
#' The `cv.glmnet` call from the glmnet package is used for fitting elastic
#' nets.  The value of $\alpha$ must be defined.  To search for a preferable
#' set of $\lambda$ and $\alpha$ values require fitting several models with
#' different $\alpha$ values and then searching for a preferable model.
#' Read the "Details" section in `help("cv.glmnet")`.
#'
# /*
# -------------------------------- Data Sets ----------------------------------
# */
#' # Data Sets
#'
#' There are two data sets provided in the ensr package for examples.
#'
#' 1. `tbi` is a synthetic data set for classifying three different types of
#' traumatic brain injury by a set of predictors.
#'
#' 2. `landfill` is a synthetic data set similar similar to ones generated by
#' computer models of water percolating through landfill.
#'
#' Detail on the construction and specifics of each of these data sets is
#' provided in the "ensr-datasets" vignette:
#'
#'```{r, eval = FALSE}
#'vignette("ensr-datasets", package = "ensr")
#'```
data(tbi, package = "ensr")
data(landfill, package = "ensr")

# /*
# ---------------------- Searching for lambda and alpha -----------------------
# */
#'
#' # Searching for $\lambda$ and $\alpha$
#'
#' Searching for a combination of $\lambda$ and $\alpha$ resulting in the lowest
#' cross validation error is done with a call to `ensr`.  The arguments to
#' `enser` are the same as those made to `cv.glment` with the addition of
#' `alphas`, a sequence of $\alpha$ values to use.  Please note that `ensr` will
#' add `length(alphas) - 1` additional values, the midpoint between the given
#' set, in the construciton of a $\lambda$--$\alpha$ grid to search.  For the
#' initial example we will fit an elastic net for modeling the evaporation in
#' the landfill data constructed above.
set.seed(42)
y_matrix <- as.matrix(landfill$evap)
x_matrix <- as.matrix(landfill[, topsoil_porosity:weather_temp])

ensr_obj <- ensr(y = y_matrix, x = x_matrix, standardize = FALSE)
ensr_obj

#'
#' The `ensr_obj` is a `ensr` object which is a list of `cv.glment` objects.
#' The length of the list is determined by the length of the $\alphas$ argument.
#' The default for `alphas` is `r deparse(as.list(args(ensr))$alphas)`.
#'
#' The summary method for `ensr` objects returns and `data.table` with value of
#' $\lambda$, $\alpha$, the mean cross-validation error `cvm`, and the number of
#' non-zero coefficients.  The `l_index` the the list index of the `ensr` object
#' associated with the noted $\alpha$ value.
ensr_obj_summary <- summary(object = ensr_obj)
ensr_obj_summary

#'
#' The preferable model is the one with the minimum cross-validation error.
ensr_obj_summary[cvm == min(cvm)]

#'
#' A quick way to get the preferable model is to call the `preferable` method.
str( preferable(ensr_obj), max.level = 1L)

#'
#' The return is a `elnet` `glmnet` object with one additional list element, the
#' `ensr_summary` used to seledc this preferable model.
#'
#' Since the return of `preferable` inherits the same class as a the objects
#' returned from a call to `glmnet::glmnet` the same methods can be used, for
#' examle, plotting
par(mfrow = c(1, 3))
plot(preferable(ensr_obj), xvar = "norm")
plot(preferable(ensr_obj), xvar = "lambda")
plot(preferable(ensr_obj), xvar = "dev")

#'
#'
plot(ensr_obj) +
  ggplot2::theme_bw() +
  ggforce::facet_zoom(x = 0.50 < alpha & alpha < 0.90, y = 5e-4 < lambda & lambda < 1.5e-3)

summary(ensr_obj)
coef(ensr_obj)










#'
#' ## Cross-Validation Issues:
#'
#' This is something we need to look into. We will perform the search for a
#' preferable model with two different foldid vectors, both set up for 10-fold
#' cross validation.
foldid1 <- sample(seq(10), size = nrow(x_matrix), replace = TRUE)
foldid2 <- sample(seq(10), size = nrow(x_matrix), replace = TRUE)
foldid3 <- sample(seq(10), size = nrow(x_matrix), replace = TRUE)

#'
#' We will fit three ensr objects and report the smallest cvm
ensr_obj_1 <- ensr(y = y_matrix, x = x_matrix, standardize = FALSE, alphas = seq(0.05, 0.95, by = 0.1), foldid = foldid1)
ensr_obj_2 <- ensr(y = y_matrix, x = x_matrix, standardize = FALSE, alphas = seq(0.05, 0.95, by = 0.1), foldid = foldid2)
ensr_obj_3 <- ensr(y = y_matrix, x = x_matrix, standardize = FALSE, alphas = seq(0.05, 0.95, by = 0.1), foldid = foldid3)

summary(ensr_obj_1)[cvm == min(cvm)]
summary(ensr_obj_2)[cvm == min(cvm)]
summary(ensr_obj_3)[cvm == min(cvm)]

#'
#' There are small differences in the cross validation errors and the in the
#' $\lambda$ values.  There is a large difference, in the $\alpha$ values.  It
#' is notable, that the differences in the regression coefficients is minor.
#' In this case there are the same number of non-zero coefficients and the
#' magnitudes are similar.
cbind(coef(ensr_obj_1), coef(ensr_obj_2), coef(ensr_obj_3))

#'
#' Consider a different output, percolation through the topsoil.
ensr_obj_1 <- ensr(x = x_matrix, y = as.matrix(landfill$perc_topsoil), standardize = FALSE, foldid = foldid1)
ensr_obj_2 <- ensr(x = x_matrix, y = as.matrix(landfill$perc_topsoil), standardize = FALSE, foldid = foldid2)
ensr_obj_3 <- ensr(x = x_matrix, y = as.matrix(landfill$perc_topsoil), standardize = FALSE, foldid = foldid3)

summary(ensr_obj_1)[cvm == min(cvm)]
summary(ensr_obj_2)[cvm == min(cvm)]
summary(ensr_obj_3)[cvm == min(cvm)]

#'
#' One could argue there is a major differnce in the result between the two
#' foldid.  Using the cve.min, the foldid1 leads to 19 non-zero coefficients
#' whereas foldid2 leads to 22 non-zero coefficients.
#'

############################## EXPLORING ###########################

#+ eval = FALSE
# newwer dev work
x <- Xmat <- model.matrix( ~ . - injury1 - injury2 - injury3 - 1, data = tbi)
y <- Yvec <- matrix(tbi$injury1, ncol = 1)

out <- ensr(Yvec, Xmat, standardize = TRUE)
str(out, max.level = 1)

summary(out)

sout <- summary(out)
sout[, z := standardize(cvm, stats = list(center = "median", scale = "IQR"))]
sout[, z := standardize(cvm, stats = list(center = "mean", scale = "sd"))]
sout[, z := standardize(cvm, stats = list(center = "min", scale = "sd"))]

library(ggplot2)
ggplot(sout) +
  aes(x = alpha, y = lambda, z = log(z), color = log(z)) +
  # aes(x = alpha, y = lambda, z = nzero, color = factor(nzero)) +
  geom_point() +
  geom_contour() +
  scale_y_log10() +
  # geom_text(mapping = aes(label = nzero, color = nzero)) +
  geom_point(data = sout[cvm == min(cvm), ], color = "red") +
  scale_color_gradient2()

standardize(summary(out)$cvm, stats = list(center = "median", scale = "IQR"))



#+ eval = FALSE
# Make a lambda_max function
# lambda_max <- function(y, x, alpha) {
#   # y <- scale(y)
#   # x <- apply(x, 2, scale)
#   N <- nrow(x)
#   # if (isTRUE(all.equal(0, alpha))) alpha <- 0.001
#   # if (isTRUE(all.equal(1, alpha))) alpha <- 0.999
#   # max(t(x) %*% (y - mean(y) * (1 - mean(y)))) / (alpha * N)
#   rbind(
#   max( abs( t( (y - mean(y) * (1 - mean(y))) )  %*% x) ) / (alpha * N)
#   max( abs( t( (y ))   %*% x) ) / (alpha * N)
#   )
#   # max( abs( t(y)  %*% x) ) / (alpha * N)
# }
# 
# lambda_max(y = matrix(tbi$injury1, ncol = 1),
#            x = model.matrix( ~ . - injury1 - injury2 - injury3 - 1, data = tbi),
#            alpha = c(0.001, .5, 1))
# 
# YVEC <-  matrix(tbi$injury1, ncol = 1)
# XMAT <- model.matrix( ~ . - injury1 - injury2 - injury3 - 1, data = tbi)
# f <- lm(YVEC ~  XMAT)
# 
# max(abs(t(XMAT) %*% (residuals(f) + predict(f, type = "terms")))) / (0.5 * nrow(XMAT))
# max(abs(t(apply(XMAT, 2, scale)) %*% (residuals(f) + predict(f, type = "terms")))) / (0.5 * nrow(XMAT))
# g <- glmnet(y = YVEC, x = XMAT, standardize = FALSE, alpha = 0.5)
# max(g$lambda)
# max(update(g, standardize = TRUE)$lambda)
# 
# coef(g)
# 
# n <- 500
# p <- 3
# b <- c(-5,3,2,0)
# b <- c(3,2,0)
# 
# X <- cbind(rep(1,n),scale(matrix(rnorm(p*n),nrow=n)))
# X <- cbind(scale(matrix(rnorm(p*n),nrow=n)))
# Y <- rbinom(n,1,prob = exp(X%*%b)/(1 + exp(X%*%b)))
# 
# alpha <- .5
# 
# max( abs(t(Y - mean(Y)*(1-mean(Y))) %*% X ) )/ ( alpha * n) # largest lambda value
# eg_fit <- glmnet(x=X,y=Y,family="poisson",alpha = alpha,standardize=FALSE)
# eg_fit$lambda[1] # largest lambda value
# lambda_max(Y, X, 0.5)
# 
# coef(eg_fit)
# 
# f <- lm(Y ~ X)
# 
# max(abs(residuals(f) - predict(f, type = "terms"))) / (alpha * n)
# 
# sum(X[1, ] *  coef(f)[-1] )
# predict(f, type = "terms")[1]
# 
# max(abs(t(residuals(f) - predict(f, type = "terms"))) %*% X) / (alpha * n)


############################## EXPLORING ###########################
## Need to set up a grid of lambda and alpha values to work with and use LOOCV
fit0 <- glmnet(x = model.matrix( ~ . - injury1 - injury2 - injury3 - 1, data = tbi),
               y = matrix(tbi$injury1, ncol = 1),
               family = "binomial",
               standardize = FALSE,
               alpha = 0)
fit1 <- update(fit0, alpha = 1)
fit5 <- update(fit0, alpha = 0.5)
max(fit0$lambda)
max(fit1$lambda)
max(fit5$lambda)

coef(fit0)
coef(fit1)
coef(fit5)

par(mfrow = c(3, 2))
hist(fit0$lambda)
hist(log10(fit0$lambda))
hist(fit1$lambda)
hist(log10(fit1$lambda))
hist(fit5$lambda)
hist(log10(fit5$lambda))

my_lambda <- sort(c(fit0$lambda, fit1$lambda))

str(fit0)

if (interactive()) {
  library(doMC)
  registerDoMC(cores = 9L)
}

cvfits <-
  lapply(seq(0, 1, by = 0.1),
         function(a) {
           cv.glmnet(x = model.matrix( ~ . - injury1 - injury2 - injury3 - 1, data = tbi),
                     y = matrix(tbi$injury1, ncol = 1),
                     alpha = a,
                     lambda = my_lambda,
                     nfolds = nrow(tbi),
                     grouped = FALSE,
                     family = "binomial",
                     parallel = TRUE)
         })


str(cvfits, max.level = 1L)
str(cvfits[[1]], max.level = 1L)
as.list(cvfits[[1]]$glmnet.fit$call)

library(dplyr)

results <-
  lapply(cvfits, function(x) {
           data.frame(lambda = x$lambda, cvm = x$cvm, nzero = x$nzero)
         }) %>%
  dplyr::bind_rows(.id = "alpha") %>%
  dplyr::mutate(alpha = seq(0, 1, by = 0.1)[as.integer(alpha)])

library(ggplot2)
library(ggforce)

ggplot(results) +
  aes(x = alpha, y = log10(lambda), color = cvm) +
  geom_point() +
  geom_point(data = {results %>% dplyr::group_by(alpha) %>% dplyr::filter(lambda == min(lambda))}, color = "red", size = 2)

ggplot(results) +
  theme_bw() +
  aes(x = alpha, y = log10(lambda), z = log10(cvm), color = log10(cvm))  +
  # geom_tile() +
  geom_point() +
  geom_contour() +
  # stat_density_2d(mapping = aes(colour = log10(cvm)))
  scale_colour_gradient(low = "blue", high = "red")

ggplot(results) +
  aes(x = nzero, y = cvm, group = factor(alpha), color = factor(alpha)) +
  geom_point() + geom_line() +
  facet_zoom(x = nzero > 10, y = cvm < 0.3)

# thought: git the lambda ranges for


alphas <- seq(0, 1, by = 0.1)
cl <- as.list(fit0$call)

glmnets <- lapply(alphas, function(a) {cl$alpha <- a; eval(as.call(cl))})

lambdas <- lapply(glmnets, `[[`, 'lambda')

goo <- function(x) {
  unlist(
  list(
  rep(x[1], times = 2)
  ,
  rep(x[-c(1, length(x))], each = 3)
  ,
  rep(x[length(x)], times = 2)
  ) )
}
goo(lambdas)


foo <- function(x) {
  firsts <- x[-length(x)]
  seconds <- x[-1]
  out <- c(2/3 *firsts + 1/3 * seconds, 1/3 * firsts + 2/3 * seconds, x)
  out <- sort(out)
  out
}
foo(alphas)

rep(foo(alphas), each = 2)


#'
# /*
# =============================================================================
# */
#'
#' # Session Info
#'
print(sessionInfo(), local = FALSE)

#'
#' # References
#'
# /*
# ------------------------------- End of File ----------------------------------
# */
